**Big Data Concepts and Terminology**

**Big Data** 
    └── Strategies and technologies for managing large datasets beyond the scope of databases.

**Evolution** 
    └── Driven by data growth in various fields, necessitating new computational approaches.


**What Is Big Data?**

**Definition** 
    └── Varies, generally large datasets and the methods to handle them.

**Characteristics** 
    └── Datasets too large for traditional methods or single computers.


**Big Data Life Cycle**

**Ingesting Data** 
    └── Importing raw data into the system.

**Persisting Data** 
    └── Storing data across distributed systems.

**Computing and Analyzing Data** 
    └── Processing data to extract insights.

**Visualizing Results** 
    └── Presenting data to highlight trends and insights.


**Clustered Computing**

**Purpose** 
    └── Addresses big data's storage and computational demands.

**Benefits** 
    └── Resource pooling, high availability, and scalability.


**Big Data Glossary**
    └── **Big Data** Datasets too large for traditional systems, requiring specific technologies and strategies.
    └── **Batch Processing** Strategy for processing large data sets in chunks, ideal for non-time-sensitive tasks.
    └── **Cluster Computing** Utilizing multiple machines' resources collectively for task completion, requiring cluster management.
    └── **Data Lake** A repository for raw, frequently changing data, differing from more structured data warehouses.
    └── **Data Mining** The practice of discovering patterns in large data sets to distill actionable information.
    └── **Data Warehouse** Ordered repositories for cleaned and integrated data, used for analysis and reporting.
    └── **ETL (Extract, Transform, Load)** Process of preparing raw data for use, common in data warehousing and big data ingestion.
    └── **Hadoop** An Apache project for big data, featuring HDFS for storage, YARN for resource scheduling, and MapReduce for batch processing.
    └── **In-memory Computing** Processing data within a cluster's memory for speed, avoiding disk I/O bottlenecks.
    └── **Machine Learning** Designing systems that improve from data input, using predictive and statistical algorithms.
    └── **MapReduce** An algorithm for distributed computing, involving task division (map) and result consolidation (reduce).
    └── **NoSQL** Databases not adhering to the relational model, suited for big data due to their flexibility and scalability.
    └── **Stream Processing** Computing on data in real-time as it flows, suitable for immediate analysis and reaction.

***Additional Terms***
    └── **Veracity** Concerns about the accuracy, reliability, and trustworthiness of data sources.
    └── **Variability** The inconsistencies and flux in data flow rates, formats, and quality.
    └── **Visualization** Techniques and tools used to graphically represent data, aiding in understanding and insight generation.
    └── **Data Governance** Policies and practices ensuring data integrity, privacy, and compliance throughout its lifecycle.
    └── **Edge Computing** Processing data near its source to reduce latency and bandwidth use, increasingly relevant in IoT and real-time analytics.
    └── **Data Fabric** An architecture and set of data services providing consistent capabilities across a choice of endpoints spanning hybrid and multicloud environments.


**Conclusion**
    └── Big data systems offer unique insights by processing data at a scale not possible with traditional methods, emphasizing the importance of efficient data management and analysis strategies.